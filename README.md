# USLT — Unified Sign Language Translator
Добро пожаловать в **USLT** — лёгкий, понятный и дружелюбный инструмент для распознавания русского жестового языка с помощью обученной модели KNN.  
Это социальный проект, направленный на интеграцию людей, которые общаются на жестовом языке, в повседневную жизнь.

# МИНИМАЛЬНЫЕ СИСТЕМНЫЕ ТРЕБОВАНИЯ
Для стабильной работы USLT рекомендуется иметь:
- **Операционная система:** Linux, macOS или Windows  
- **Python:** версия 3.8 или выше  
- **RAM:** минимум 4 GB  
- **CPU:** минимум частота - 2.0 GHz, 2 ядра, архитектура x86_64/amd64. Поддержка SIMD не требуется. GPU не обязателен, CPU достаточно для работы модели KNN
- **Дополнительное ПО:** pip для установки зависимостей, система контроля версий git
- **Дисковое пространство:** минимум 200 MB для проекта и моделей. Если у вас не установлен Python, дополнительное ПО или какие-то зависимости, то необходимое дисковое пространство увеличивается
- **Ввод информации:** исправная веб-камера

# УСТАНОВКА ПРОЕКТА
**1 шаг:** клонирование репозитория
Создайте папку, в которой будет храниться проект и перейдите в неё в терминале. После этого выполните следующую команду:
```bash
git clone https://github.com/reevl1/USLT.git
cd USLT
```

**2 шаг:** установка зависимостей
**Linux/macOS**
```bash
python3 -m venv venv
source venv/bin/activate
pip3 install opencv-python mediapipe numpy scikit-learn joblib Pillow
```

**Windows (cmd)**
```bash
python -m venv venv
venv\Scripts\activate
pip install opencv-python mediapipe numpy scikit-learn joblib Pillow
```

**Windows (PowerShell)**
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install opencv-python mediapipe numpy scikit-learn joblib Pillow
```

**3 шаг:** запуск скрипта<br>
Поскольку мы создали виртуальное окружения, то работа скрипта будет всегда происходить через него. Ниже представлен код, который нужно исполнить для активации виртуально окружения и запуска стандартной работы программы **после** того как Вы в терминале перейдёте в нужную папку
## Linux / macOS
```bash
source venv/bin/activate
python3 uslt.py --mode run
```

## Windows (cmd)
```bash
venv\Scripts\activate
python uslt.py --mode run
```

## Windows (PowerShell)
```bash
.\venv\Scripts\Activate.ps1
python uslt.py --mode run
```

# РЕЖИМЫ РАБОТЫ
Параметр **--mode** обзятелен и моежет принимать и другие аргументы.

**collect** - собирает данные для модели, имеет обязательный параметр **--letter**, который говорит какую букву мы хотим собрать в режиме **collect**, в остальных режимах не используется
Пример:
```bash
python3 uslt.py --mode collect --letter А
```
1) Запустится веб-камера.
2) Показывайте жест буквы "А" (или другой буквы, которая предана как аргумент параметра **--letter**), нажимайте "c" для сохранения кадра.
3) "q" — выход из режима.
4) Данные сохраняются в sign_letters_dataset.csv

**train** - обучает модель, требует минимум 2 буквы для обучения.
Пример:
```bash
python3 uslt.py --mode train
```
1) Скрипт проверяет наличие датасета sign_letters_dataset.csv.
2) Разбивает данные на тренировочную и тестовую выборки.
3) Обучает модель KNN.
4) Выводит accuracy и отчёт classification_report.
5) Сохраняет модель в sign_letters_knn.pkl.

**run** - распознаёт жесты в реальном времени, базовый сценарий исполнения программы.
Пример:
```bash
python3 uslt.py --mode run
```
1) Загружает сохранённую модель sign_letters_knn.pkl.
2) Запускает веб-камеру для распознавания жестов в реальном времени.
3) Отображает текущую букву и собранный текст прямо на видео.
4) "q" — выход из режима.

# СТРУКТУРА ПРОЕКТА
**Файл**	                |   **Назначение**<br>
uslt.py	                  |   Основной скрипт: загружает модель, обрабатывает данные и выдаёт текст<br>
sign_letters_dataset.csv  |	  Датасет с признаками жестов и соответствующими буквами<br>
sign_letters_knn.pkl	    |   Обученная модель KNN (pickle-файл) для распознавания жестов<br>
README.md	                |   Документация проекта<br>

# БУДУЩЕЕ ПРОЕКТА
На данный момент программа распознаёт статичные отдельные буквы.
**В планах команды:**
* Распознавание более сложных жестов (последовательные движения).
* Поддержка нескольких говорящих одновременно.
* Удобный пользовательский интерфейс, включая мобильные приложения.
* Добавление других жестовых языков для расширения функционала (но это уже более далёкие планы).
